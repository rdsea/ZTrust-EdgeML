- dont need to 

**Method** lam j 
- how to prove 
  - steps: 1, 2, 3 ... (pipeline)
    - metrics

- Hong-Tri: working much on methods and read more papers related to benchmarking
- Dung: 
- Chau: 

## Utilities
1. utilities/ for all benchmark? what is the current?
  - deploy ziti and
  - if 3 utilities
    - setup **infrastructure**
    - scenario a. setup edge, cloud, sth --> how to have a setup
    - experiments 

1. utilities for **testing**
2. how to add/extend utilities 

- setup: edge and cloud
- tech: k8s or docker
- providers: connection between devices and providers?
- adding ZT networking? 

- having an application on top of that, what happens? what could test?
- [metrics] why that is important?
- [extension] replace openziti with another one? what should? 

## METHOD **Reflection** 
1. setup ziti with infrastructure
2. setup applications on 
3. cloud/edge and applications -> metrics and attach with ML and why important
4. Test? loss/performance and extension (metrics, ML applications, infrastructures, stakeholders)

- single/multi providers
- technologies

## read guideline 
- page 6 until ending with appendix and references
  - add some scripts for that if needed
- experiments and methods


## how to reflect experiments and methods  
- if add another zt tools how to change?

With many setups, how to have common utilities and common configuration?
- specs to point setups
- if two setups 
- configuration?

The difference of methods reflect to the difference from github setting 

3. edge-cloud in the same providers
4. edge-cloud in difference providers
5. Infrastructure edge and cloud 
6. edge/cloud provision space/ stakeholders/ vm technology/ zerotrust config/ 

- generate many views as we can
- deploy and test
- How to ? 

- Hong-Tri: 
