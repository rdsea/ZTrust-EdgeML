#!/bin/bash

set -euo pipefail

source "$(dirname "$0")/common.sh"


# Assumption
# It is not normally necessary to obtain publicly trusted certificates for Ziti's TLS servers.
# Ziti manages the trust relationships between the controller and routers and clients independent of the web's root authorities.
# See the Alternative Web Server Certificates section for more information.


setting_k8s(){
eval "$(cd ../gke/ && terraform output -raw kubeconfig_command)"
CLUSTER_IP=$(cd ../gke/ && terraform output -raw {{ gke.cluster_name | replace('-', '_') }}_ip)
CTRL_ADVERTISE="{{ ziti_config.ctrl.cloud_ctrl.ctrl_advertised_address }}"
ROUTER_NAMESPACE="router"

# Create namespace if it doesn't exist
CTRL_NAMESPACE="{{ gke.namespace }}"

ROUTER_ADVERTISE="{{ ziti_config.ctrl.cloud_ctrl.router.cloud_router_advertised_address }}"
ROUTER_ID="ziti-{{ ziti_config.ctrl.cloud_ctrl.router.id | replace('_', '') }}"

if ! kubectl get namespace "$CTRL_NAMESPACE" >/dev/null 2>&1; then
  echo "Creating namespace: $CTRL_NAMESPACE"
  kubectl create namespace "$CTRL_NAMESPACE"
else
  echo "Namespace $CTRL_NAMESPACE already exists."
fi


  }

trust_setup(){
helm repo add jetstack https://charts.jetstack.io

log "cert-manager"
helm upgrade --install cert-manager jetstack/cert-manager \
    --namespace cert-manager --create-namespace \
    --set crds.enabled=true

wait_for_deployment cert-manager cert-manager


log "trust-manager"
helm upgrade --install trust-manager jetstack/trust-manager \
    --namespace cert-manager \
    --set crds.keep=false \
    --set app.trust.namespace=$CTRL_NAMESPACE

wait_for_deployment cert-manager trust-manager


log "zt-ctrl"
helm install ziti-controller openziti/ziti-controller \
  --namespace $CTRL_NAMESPACE \
  --set cert-manager.enabled=false \
  --set trust-manager.enabled=false \
  --set clientApi.advertisedHost="$CTRL_ADVERTISE" \

wait_for_deployment "$CTRL_NAMESPACE" ziti-controller

  }

setup_dns(){
# DNS setting 
env PUB_CTRL_IP="$PUB_CTRL_IP" envsubst < gke_dns_configmap.yml | kubectl apply -f -

wait_for_deployment kube-system  custom-dns-server

CUSTOM_DNS_IP=$(kubectl get pod -n kube-system -l app=custom-dns-server -o jsonpath='{.items[0].status.podIP}')

kubectl patch configmap kube-dns -n kube-system --type merge -p "{\"data\":{\"stubDomains\":\"{\\\"{{ custom_domain }}\\\":[\\\"$CUSTOM_DNS_IP\\\"]}\"}}"

kubectl delete pods -n kube-system -l k8s-app=kube-dns

wait_for_deployment kube-system  custom-dns-server

}

setup_jaeger(){

log "jaeger-setup"
kubectl create ns observability
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml -n observability

# Jaeger with in-momory storage
kubectl apply -f - <<EOF
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: jaeger-inmemory-instance
  namespace: observability
spec:
  image: jaegertracing/jaeger:latest
  ports:
  - name: jaeger
    port: 16686
  config:
    service:
      extensions: [jaeger_storage, jaeger_query]
      pipelines:
        traces:
          receivers: [otlp]    
          exporters: [jaeger_storage_exporter]
    extensions:
      jaeger_query:
        storage:
          traces: memstore
      jaeger_storage:
        backends:
          memstore:
            memory:
              max_traces: 100000
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    exporters:
      jaeger_storage_exporter:
        trace_storage: memstore
EOF

# setting sidecar
kubectl apply -f - <<EOF
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: sidecar-for-my-app
spec:
  mode: sidecar
  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact: {}
    processors:
      batch:
        send_batch_size: 10000
        timeout: 5s

    exporters:
      debug: {}

    service:
      pipelines:
        traces:
          receivers: [jaeger]
          exporters: [debug]
EOF

kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  type: LoadBalancer
  selector:
    app: jaeger-inmemory-instance  # <- Ensure this label matches pod
  ports:
    - name: grpc
      port: 4317
      targetPort: 4317
    - name: http
      port: 4318
      targetPort: 4318
EOF
  }


main(){

  setting_k8s
  trust_setup

  wait_for_ip_and_advertise PUB_CTRL_IP CTRL_ADVERTISE \
    "kubectl get svc ziti-controller-client -n \"$CTRL_NAMESPACE\" -o jsonpath='{.status.loadBalancer.ingress[0].ip}'" \
    $CTRL_ADVERTISE
  
  CTRL_PASS=$(kubectl -n "$CTRL_NAMESPACE" get secrets ziti-controller-admin-secret -o go-template='{{"{{"}}index .data "admin-password" | base64decode {{"}}"}}')

  setup_localhost "$PUB_CTRL_IP" "$CTRL_ADVERTISE"
  login_zt "$CTRL_ADVERTISE" "$CTRL_PASS"
  setup_dns
  setup_router "$ROUTER_ID" "$ROUTER_NAMESPACE" "$ROUTER_ADVERTISE"

  wait_for_ip_and_advertise PUB_ROUTER_IP ROUTER_ADVERTISE \
    "kubectl get svc \"$ROUTER_ID-edge\" -n \"$ROUTER_NAMESPACE\" -o jsonpath='{.status.loadBalancer.ingress[0].ip}'" \
    $ROUTER_ADVERTISE

  setup_localhost "$PUB_ROUTER_IP" "$ROUTER_ADVERTISE"
  log "Edge and Sensor deployment complete."

  export PUB_CTRL_IP PUB_ROUTER_IP CTRL_PASS
  envsubst '${PUB_CTRL_IP} ${PUB_ROUTER_IP} ${CTRL_PASS}' < ../k3s/k3s_edge_cluster.sh.tmpl > ../k3s/k3s_edge_cluster.sh
  envsubst '${PUB_CTRL_IP} ${PUB_ROUTER_IP} ${CTRL_PASS}' < ../k3s/k3s_edge_setup_router.sh.tmpl > ../k3s/k3s_edge_setup_router.sh
}

main "$@"


